{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c24d9c",
   "metadata": {},
   "source": [
    "# Optimization and MCMC\n",
    "\n",
    "Gareth Funning, University of California, Riverside\n",
    "\n",
    "Here are examples of using optimization methods (e.g. the Powell algorithm, as included in the SciPy library) and a Markov Chain Monte Carlo $-$ MCMC $-$ method (e.g. the Metropolis algorithm) to solve nonlinear inverse problems. This is pretty fancy stuff $-$ but you should take care that the results you get are sensible and representative!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0a43aa",
   "metadata": {},
   "source": [
    "## 0. Dependencies\n",
    "\n",
    "Again, make sure you're in the correct conda environment, or this won't work (and you'll have to quit out of this and activate it, and then restart Jupyter...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ffb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your dependencies\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize, Bounds\n",
    "import pygmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc9229",
   "metadata": {},
   "source": [
    "## 1. Some functions\n",
    "\n",
    "We will reintroduce our 1D slip model here, alongside a couple of other functions that make use of it. One is a penalty function, that computes the total squared misfit. The other is the log-likelihood (or 'log-probability') function, which, it turns out, is just the total squared misfit, $RSS \\times -2$.\n",
    "\n",
    "Note that we put all of the model parameters in a single vector (a numpy array) $-$ this is because the different approaches we will use below expect that, and expect it to be the first input to the penalty and log-probability functions.\n",
    "\n",
    "We will use these in the exercises below. Optimization methods will try to minimize the penalty function, whereas MCMC methods will try to maximize the log-probability function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our 1d slip model as before\n",
    "def slip1d(s, D, v_shift, x):\n",
    "    v=(s/np.pi)*np.arctan(x/D)+v_shift\n",
    "    return v\n",
    "\n",
    "# the total squared penalty function\n",
    "# 'state' here is an array of the model parameters - s, D and v_shift\n",
    "def penalty(state, x, data):\n",
    "    s=state[0]\n",
    "    D=state[1]\n",
    "    v_shift=state[2]\n",
    "    v=slip1d(s, D, v_shift, x)\n",
    "    p=np.sum((data-v)**2)\n",
    "    return p\n",
    "    \n",
    "# the log probability function (-2 * the total squared misfit)\n",
    "def logprobability(state, x, data):\n",
    "    p=penalty(state, x, data)\n",
    "    logp=-2*p\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee592443",
   "metadata": {},
   "source": [
    "## 2. Data and model setup\n",
    "\n",
    "Here, we will load in the data we used before, and set the upper and lower parameter bounds for the modeling we want to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90acaf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data!\n",
    "\n",
    "# i took the liberty of just outputting the arrays we used last time\n",
    "# (minus the obvious outlier)\n",
    "\n",
    "x_gps=np.array((-20.12591147,  141.0701294 ,   78.25686249,  -55.40876592,\n",
    "       -103.8866626 ,    9.42240444,   38.98726886,\n",
    "         14.22365707,  -79.80173625,  -64.04128843,    2.88787778,\n",
    "        -81.4305956 ,  -30.8734159 ,    7.18245415,  -69.81071468,\n",
    "         21.67544474,   -2.26853432, -106.16939644,  -15.30398516,\n",
    "          2.02005263, -106.48341489))\n",
    "\n",
    "v_gps=np.array((-37.35777473, -16.7157987 , -16.62951891, -42.66468414,\n",
    "       -47.93899115, -25.60814096, -17.06184387,\n",
    "       -23.93251853, -48.90754722, -46.93080769, -30.11640353,\n",
    "       -46.82690154, -41.18189799, -26.69292887, -46.48615705,\n",
    "       -20.49406783, -33.3267652 , -47.59029564, -37.38865282,\n",
    "       -32.67695259, -47.6475494))\n",
    "\n",
    "v_gps_err=np.array((1.48695262, 2.52997939, 0.5908995 , 0.62248426, 1.88520919,\n",
    "       1.34222111,  0.41498951, 0.44405611, 1.34222111,\n",
    "       1.68551465, 0.4       , 1.3       , 1.3       , 0.73887019,\n",
    "       1.4       , 0.66079412, 1.3       , 1.88520919, 0.44895155,\n",
    "       1.24229384, 1.2))\n",
    "\n",
    "# let's define the parameter ranges we want to optimize over\n",
    "smin=25\n",
    "smax=45\n",
    "Dmin=10\n",
    "Dmax=25\n",
    "v_shiftmin=-38\n",
    "v_shiftmax=-28\n",
    "\n",
    "# make arrays of our upper and lower parameter bounds\n",
    "pars_lb=np.array((smin, Dmin, v_shiftmin))\n",
    "pars_ub=np.array((smax, Dmax, v_shiftmax))\n",
    "npars = len(pars_lb)  # number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1b5a9",
   "metadata": {},
   "source": [
    "## 3. Simple optimization with SciPy and the Powell algorithm\n",
    "\n",
    "The Powell algorithm is a downhill, direction-set algorithm which is probably overkill for this specific example, but it is pretty straightforward to use it. (The complicated thing, really, is to set up the model in the first place. Once you have a penalty function and bounds established, it's pretty much plug and play to run it.) The `scipy.optimize.minimize` function can be bounded, which is helpful to stop it going wildly off-base. All it requires is the penalty function, the initial state of the model (which we choose at random), the additional inputs (arguments, or `args`) to the penalty function, and the bounds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6cd025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a proper bounds object\n",
    "pars_bounds = Bounds(pars_lb, pars_ub)\n",
    "\n",
    "# choose a random initial model state\n",
    "init=pars_lb+(pars_ub-pars_lb)*np.random.rand(npars)\n",
    "\n",
    "# and run the Powell algorithm minimizer! \n",
    "results = minimize(penalty, init, args=(x_gps, v_gps),  method='Powell', \n",
    "                   bounds=pars_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6754b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and output the results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08347f4",
   "metadata": {},
   "source": [
    "Note that the results are provided in a structure. Some fields of note:\n",
    "\n",
    "- `results.x` is the best-fitting parameter values\n",
    "- `results.fun` is the best penalty\n",
    "- `results.nfev` is the number of times the penalty function was evaluated\n",
    "\n",
    "Check your results by plotting the fit of the model to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb17f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is for evaluating and plotting your best-fitting model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323b3c82",
   "metadata": {},
   "source": [
    "What happens if you run the inversion again? Do you get the same parameter values and penalty? Why might they change? How could you find a more definitive answer? Could you implement a strategy to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get on with implementing something\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1099d11e",
   "metadata": {},
   "source": [
    "## 4. A simple Metropolis algorithm for MCMC\n",
    "\n",
    "Markov Chain Monte Carlo (MCMC) approaches are effectively a random walk though parameter space, guided by probability. The most probable models are those that fit the data well. Typically, we try to maximize the log-probability of the model, which sounds difficult until you realize it is closely related to the total squared misfit (which took me years to realize, but never mind...)\n",
    "\n",
    "Specifically, here we use a Metropolis algorithm to conduct this sampling of parameter space. There are a couple of rules that we follow here:\n",
    "\n",
    "- Every iteration starts with a known set of parameters and their corresponding log-probability\n",
    "- We then propose a random perturbation to those parameters and compute the log-probability for those\n",
    "- If the probability of the proposed model is higher, we accept the new parameters and start a new iteration\n",
    "- If the probability of the proposed model is lower, we draw a (log) random number and compare it to the (log) ratio of the probabilities of the two models; if the ratio is the larger of the two, then we accept the new parametes and start a new iteration\n",
    "- If the new parameters do not pass these tests, we retain the original parameters and start a new iteration\n",
    "\n",
    "Over multiple iterations, this will move the model parameters towards the part of parameter space that contains the 'good' models (ones that fit the data). At least, I hope so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8629196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will apply a very simple metropolis algorithm\n",
    "# this is based on Iain Murray's 'dumb metropolis'\n",
    "# - see here: https://homepages.inf.ed.ac.uk/imurray2/teaching/09mlss/\n",
    "# - and here: https://homepages.inf.ed.ac.uk/imurray2/teaching/09mlss/slides.pdf\n",
    "\n",
    "# define what we shall use as our inputs \n",
    "x=x_gps\n",
    "data=v_gps\n",
    "ndata=len(data)\n",
    "\n",
    "# choose a random initial model state\n",
    "init=pars_lb+(pars_ub-pars_lb)*np.random.rand(npars)\n",
    "\n",
    "# some parameters for your metropolis run\n",
    "niters=10000    # number of iterations\n",
    "stepsigma=1  # a scaling for the size of possible parameter changes\n",
    "               # (can be a scalar or an array of length npars)\n",
    "\n",
    "# somewhere to store the output\n",
    "samples=np.empty((niters,npars))    # all your accepted model states \n",
    "logprobs=np.empty((niters))         # all your log penalties\n",
    "arate=0                            # acceptance counter\n",
    "\n",
    "# ok, lets's get going\n",
    "state = init                           # current model state\n",
    "lp_state = logprobability(state, x, data)  # current log penalty\n",
    "\n",
    "# loop through all your iterations\n",
    "for ss in np.arange(niters):\n",
    "    \n",
    "    # propose a new model state and calculate its log probability\n",
    "    prop = state + stepsigma*np.random.randn(npars);\n",
    "    lp_prop = logprobability(prop, x, data);\n",
    "    \n",
    "    # test for acceptance\n",
    "    if np.log(np.random.rand(1)) < (lp_prop-lp_state):\n",
    "        # if you accept the change, update your counter, model state and log probability\n",
    "        arate+=1\n",
    "        state = prop\n",
    "        lp_state = lp_prop\n",
    "    \n",
    "    # and record the result of the iteration for posterity\n",
    "    samples[ss,:] = state\n",
    "    logprobs[ss]= lp_state\n",
    "\n",
    "arate = arate/niters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b52b2d",
   "metadata": {},
   "source": [
    "Note that we store all of the 'accepted' model parameters in an array, which means we can analyze them as an ensemble to see how they did. We can start by simply taking the mean, but we really ought to plot them and look at some histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37cffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean s: ',np.average(samples[:,0]))\n",
    "print('mean D: ',np.average(samples[:,1]))\n",
    "print('mean v_shift: ',np.average(samples[:,2]))\n",
    "print('fraction of accepted models: ',arate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637cf63d",
   "metadata": {},
   "source": [
    "Consider plotting:\n",
    "\n",
    "- the penalty function as it evolves over all of the iterations\n",
    "- the parameter values as they evolve over all of the iterations\n",
    "- histograms of the parameter values\n",
    "- maybe a scatter plot between s and D\n",
    "\n",
    "Do you see any differences between early models and late ones? Which ones might be more representative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this space for thinking about some plots...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19fb9c3",
   "metadata": {},
   "source": [
    "Note also that the algorithm takes as an input a value, called here `stepsigma` which is a scaling of the step size. What happens if you change it, and re-run the inversion? What else might you want to change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe copy the MCMC code here and re-run with different input values, and re-plot the results?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
